{"cells":[{"metadata":{"_uuid":"e2502fa6-d47c-4cdb-8130-33c5c32f9c56","_cell_guid":"03634998-075d-45b3-bbf2-7be5c6deb17a","trusted":true},"cell_type":"code","source":"# %% [code]\nPARQUETS_DIR = f'/kaggle/input/parquets/'\nMODELS_DIR = f'/kaggle/input/riiid-answer-correctness-prediction-models/'\n\nOUT_DIR = '/kaggle/working/'\n\n# %% [code]\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom shutil import copyfile\nimport stacking_classifiers as sc\nfrom sklearn.ensemble import RandomForestClassifier\n\n# %% [code]\ndef get_data():\n    data = pd.read_parquet(PARQUETS_DIR + 'train_df.parquet', columns=FEATURES+[TARGET])\n    train, valid = split_train_valid(data, val_size=1_000_000).values()\n    \n    return train, valid\n\n# %% [code]\ndef split_train_valid(dt, val_size):       \n    val = dt.iloc[-val_size:]\n    trn = dt.iloc[:-val_size]\n    xtrn, ytrn = trn.drop(columns=[TARGET]), trn[TARGET]\n    xval, yval = val.drop(columns=[TARGET]), val[TARGET]\n    \n    return {'trn': {'x': xtrn, 'y': ytrn},\n            'val': {'x': xval, 'y': yval}}\n\n# %% [code]\ndef optimize(clf, label, init_points=10, cont_opt=False, version='v1', **kwargs):\n    opt_filename = f'{label}_opt.json'\n    if cont_opt and not os.path.exists(OUT_DIR+opt_filename):\n        copyfile(MODELS_DIR+f'{label}/{version}/'+opt_filename, OUT_DIR+opt_filename)\n\n    train, valid = get_data()\n    scorer = sc.get_scorer(clf, train, valid, fixed_params, opt_dtypes)\n    optimizer = sc.get_optimizer(scorer,\n                                 params_grid=opt_grid,\n                                 probes=probes,\n                                 cont_opt=cont_opt,\n                                 filename=OUT_DIR+opt_filename,\n                                 **kwargs)\n    optimizer.maximize(init_points=init_points, n_iter=200, alpha=1e-6)\n\n# %% [code]\nTARGET = 'answered_correctly'\nFEATURES = [\n#     'user_id',\n#     'content_id',\n    'prior_question_elapsed_time',\n#     'prior_question_had_explanation',\n    'part',\n#     'tag1',\n#     'tag2',\n#     'tag3',\n#     'tag4',\n#     'tag5',\n#     'tag6',\n    'user_id_count',\n    'user_id_mean',\n    'user_id_attempts',\n    'content_id_count',\n    'content_id_mean',\n    'tag_count_0',\n    'tag_count_1',\n    'tag_count_2',\n    'tag_count_3',\n#     'tag_count_4',\n#     'tag_count_5',\n    'tag_mean_0',\n    'tag_mean_1',\n    'tag_mean_2',\n    'tag_mean_3',\n#     'tag_mean_4',\n#     'tag_mean_5',\n    'user_id_tag_count_0',\n    'user_id_tag_count_1',\n    'user_id_tag_count_2',\n    'user_id_tag_count_3',\n#     'user_id_tag_count_4',\n#     'user_id_tag_count_5',\n    'user_id_tag_mean_0',\n    'user_id_tag_mean_1',\n    'user_id_tag_mean_2',\n    'user_id_tag_mean_3',\n#     'user_id_tag_mean_4',\n#     'user_id_tag_mean_5',\n    'user_content_hmean',\n#     'tags_hmean',\n    'tags_whmean',\n#     'user_tags_hmean'\n    'user_tags_whmean'\n]\n\nCAT_FEATURES = [\n    'part'\n]\n\n# %% [markdown]\n# # Hyperparameter Optimization\n\n# %% [markdown]\n# ## LGBM\n\n# %% [code]\n# Fixed LGBM parameters:\nfixed_params = {\n    'num_iterations' : 100,\n    'metric': 'auc',\n    'objective': 'binary',\n    'boosting_type': 'gbdt',\n    'is_unbalance': True,\n    'num_threads' : 2,\n    'verbose': 0\n}\n\nopt_grid = {\n    'learning_rate': (0.001, 0.2),\n    'feature_fraction': (0.5, 0.9),\n    'lambda_l2': (0, 5),\n    'num_leaves': (50, 1500),\n    'min_data_in_leaf': (10, 2000)\n}\n\nopt_dtypes = {\n    'learning_rate': float,\n    'feature_fraction': float,\n    'lambda_l2': float,\n    'num_leaves': int,\n    'min_data_in_leaf': int\n}\n\nprobes = [\n    {\"feature_fraction\": 0.689624417022823,\n     \"lambda_l2\": 4.450308176307347,\n     \"learning_rate\": 0.12494856029217047,\n     \"min_data_in_leaf\": 1513,\n     \"num_leaves\": 741}\n]\n\n# %% [code]\nif False:\n    optimize(clf=sc.LGBMClassifier,\n             label='lgbm',\n             init_points=10,\n             cont_opt=True,\n             use_transformer=False)\n\n# %% [markdown]\n# ## XGBoost\n\n# %% [code]\n# Fixed LGBM parameters:\nfixed_params = {\n    'nrounds' : 50,\n    'eval_metric': 'auc',\n    'objective': 'binary:logistic',\n    'tree_method': 'exact',\n    'num_parallel_tree': 1,\n    'verbosity': 0\n}\n\nopt_grid = {\n    'learning_rate': (0.001, 0.2),\n    'colsample_bytree': (0.5, 0.9),\n    'subsample': (0.5, 1),\n    'lambda': (1, 100),\n    'max_depth': (5, 7.99),\n    'min_child_weight': (1, 2000)\n}\n\nopt_dtypes = {\n    'learning_rate': float,\n    'colsample_bytree': float,\n    'subsample': float,\n    'lambda': float,\n    'max_depth': int,\n    'min_child_weight': int\n}\n\nprobes = [\n    {\"learning_rate\": 0.12494856029217047,\n     \"colsample_bytree\": 0.689624417022823,\n     \"subsample\": 1,\n     \"lambda\": 4.450308176307347,\n     \"max_depth\": 6,\n     \"min_child_weight\": 1513},\n    {\"learning_rate\": 0.18,\n     \"colsample_bytree\": 0.5014,\n     \"subsample\": 0.7202,\n     \"lambda\": 1.654,\n     \"max_depth\": 6,\n     \"min_child_weight\": 348}\n]\n\n# %% [code]\nif False:\n    optimize(clf=sc.XGBClassifier,\n             label='xgb',\n             init_points=10,\n             cont_opt=True,\n             use_transformer=False,\n             version='v1')\n\n# %% [markdown]\n# ## CatBoost\n\n# %% [code]\n# Fixed CatBoost parameters:\nfixed_params = {\n    'iterations' : 50,\n    'bootstrap_type': 'Bernoulli',\n    'eval_metric': 'AUC',\n    'grow_policy': 'Lossguide', \n    'allow_writing_files': False,\n    'od_type': 'Iter',\n    'auto_class_weights': 'Balanced',\n    'use_best_model': True\n}\n\nopt_grid = {\n    'learning_rate': (0.001, 0.2),\n    'depth': (8, 16.99),\n    'l2_leaf_reg': (1, 100),\n    'colsample_bylevel': (0.5, 1),\n    'subsample': (0.6, 1),\n    'min_data_in_leaf': (1, 2000),\n    'max_leaves': (50, 2000)\n}\n\nopt_dtypes = {\n    'learning_rate': float,\n    'depth': int,\n    'l2_leaf_reg': float,\n    'colsample_bylevel': float,\n    'subsample': float,\n    'min_data_in_leaf': int,\n    'max_leaves': int\n}\n\nprobes = [\n    {\"learning_rate\": 0.12494856029217047,\n     \"colsample_bylevel\": 0.689624417022823,\n     \"subsample\": 0.9,\n     \"l2_leaf_reg\": 4.450308176307347,\n     \"depth\": 6,\n     \"min_data_in_leaf\": 1513,\n     \"max_leaves\": 741},\n]\n\n# %% [code]\nif False:\n    optimize(clf=sc.CATBClassifier,\n             label='catb',\n             init_points=10,\n             cont_opt=True,\n             use_transformer=False,\n             version='v1')\n\n# %% [markdown]\n# ## Random Forest\n\n# %% [code]\n# Fixed Random Forest parameters:\nfixed_params = {\n    'n_jobs': 2,\n    'clf_type': RandomForestClassifier\n}\n\nopt_grid = {\n    'max_depth': (6, 16.99),\n    'n_estimators': (10, 1000),\n    'max_features': (0.5, 1),\n    'max_samples': (0.5, 1),\n    'min_samples_leaf': (0.05, 0.3),\n    'min_samples_split': (0.05, 0.3),\n}\n\nopt_dtypes = {\n    'max_depth': int,\n    'n_estimators': int,\n    'max_features': float,\n    'max_samples': float,\n    'min_samples_leaf': float,\n    'min_samples_split': float,\n}\n\nprobes = [\n    {\"max_depth\": 6,\n     \"n_estimators\": 200,\n     \"max_features\": 0.9,\n     \"max_samples\": 0.8,\n     \"min_samples_leaf\": 0.1,\n     \"min_samples_split\": 0.1}\n]\n\n# %% [code]\nif 1:\n    optimize(clf=sc.SklearnClassifier,\n             label='rf',\n             init_points=25,\n             cont_opt=0,\n             use_transformer=False,\n             version='v1')\n\n# %% [code]\ntrain, valid = get_data()\nparams = probes[0]\nparams.update(fixed_params)\nclf = sc.SklearnClassifier(params)\nclf.train(train, valid, True)\n\n# %% [code]\nclf.best_score\n\n# %% [markdown]\n# ## Neural Network\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [code]\n# lgbm_model = LGBMClassifier(lgbm_params)\n# xgb_model = XGBClassifier(xgb_params)\n# base_model = LogisticRegressionClassifier()\n# model = StackingClassifier(classifiers=[lgbm_model, xgb_model],\n#                            base_classifier = base_model,\n#                            nfolds=5)\n\n# data = pd.read_parquet(parquets_dir + 'train_df.parquet', columns=FEATURES+[TARGET])\n# train_data, valid_data = split_train_valid(data, val_size=1_000_000).values()\n# del data\n# gc.collect()\n\n# model.train(train_data['x'], train_data['y'])\n\n# %% [code]\n# model.score(valid_data['x'], valid_data['y'])\n\n# %% [code]\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}