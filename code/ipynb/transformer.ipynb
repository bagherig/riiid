{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"COMPILE = 0\nVERSION = 'v2'\n\nDATA_DIR = '/kaggle/input/riiid-test-answer-prediction/'\nPARQUETS_DIR = f'/kaggle/input/parquets/'\nMODELS_DIR = f'/kaggle/input/riiid-answer-correctness-prediction-models/'\n\nOUT_DIR = '/kaggle/working/'","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import some typical packages and fix few random seed.\n# Plz fix the torch seed as well...\n# NB many imports are not used, but they are just there...\nimport riiideducation \n\nimport os\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport gc\nimport time\nimport itertools\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import roc_auc_score, auc, roc_curve\nfrom sklearn.model_selection import train_test_split\n\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom time import time\nfrom collections import namedtuple\n\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ngc.enable()\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nfrom torch.nn.modules.dropout import Dropout\nfrom torch.nn.modules.activation import MultiheadAttention\nfrom torch.nn.modules.normalization import LayerNorm","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TARGET = 'answered_correctly'\nKEY_FEATURE = 'user_id'\nFEATURES = [\n    'content_id',\n    'prior_question_elapsed_time',\n    #     'prior_question_had_explanation',\n    'part',\n    #     'tag1',\n    #     'tag2',\n    #     'tag3',\n    #     'tag4',\n    #     'tag5',\n    #     'tag6',\n#     'user_id_count',\n#     'user_id_wmean',\n#     'user_id_attempts',\n#     'content_id_count',\n#     'content_id_mean',\n#     'tag_count_0',\n#     'tag_count_1',\n#     'tag_count_2',\n#     'tag_count_3',\n    #     'tag_count_4',\n    #     'tag_count_5',\n#     'tag_mean_0',\n#     'tag_mean_1',\n#     'tag_mean_2',\n#     'tag_mean_3',\n    #     'tag_mean_4',\n    #     'tag_mean_5',\n#     'user_id_tag_count_0',\n#     'user_id_tag_count_1',\n#     'user_id_tag_count_2',\n#     'user_id_tag_count_3',\n    #     'user_id_tag_count_4',\n    #     'user_id_tag_count_5',\n#     'user_id_tag_mean_0',\n#     'user_id_tag_mean_1',\n#     'user_id_tag_mean_2',\n#     'user_id_tag_mean_3',\n    #     'user_id_tag_mean_4',\n    #     'user_id_tag_mean_5',\n#     'user_content_hmean',\n    #     'tags_hmean',\n#     'tags_whmean',\n    #     'user_tags_hmean'\n#     'user_tags_whmean'\n]\n\nDTYPES = {\n    'content_id': int,\n    'prior_question_elapsed_time': int,\n    #     'prior_question_had_explanation',\n    'part': int,\n    #     'tag1',\n    #     'tag2',\n    #     'tag3',\n    #     'tag4',\n    #     'tag5',\n    #     'tag6',\n#     'user_id_count',\n#     'user_id_wmean',\n#     'user_id_attempts',\n#     'content_id_count',\n#     'content_id_mean',\n#     'tag_count_0',\n#     'tag_count_1',\n#     'tag_count_2',\n#     'tag_count_3',\n    #     'tag_count_4',\n    #     'tag_count_5',\n#     'tag_mean_0',\n#     'tag_mean_1',\n#     'tag_mean_2',\n#     'tag_mean_3',\n    #     'tag_mean_4',\n    #     'tag_mean_5',\n#     'user_id_tag_count_0',\n#     'user_id_tag_count_1',\n#     'user_id_tag_count_2',\n#     'user_id_tag_count_3',\n    #     'user_id_tag_count_4',\n    #     'user_id_tag_count_5',\n#     'user_id_tag_mean_0',\n#     'user_id_tag_mean_1',\n#     'user_id_tag_mean_2',\n#     'user_id_tag_mean_3',\n    #     'user_id_tag_mean_4',\n    #     'user_id_tag_mean_5',\n#     'user_content_hmean',\n    #     'tags_hmean',\n#     'tags_whmean',\n    #     'user_tags_hmean'\n#     'user_tags_whmean'\n}\n\nCAT_FEATURES = [\n    'part'\n]","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transformer Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_FILENAME = 'transformer_best.pth'\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nSEQ_LEN = 100\nPAD_VALUE = 0\nSTART_TOKEN = 2","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass TransformerModel(nn.Transformer):\n    def __init__(self, **params):\n        '''\n        nhead -> number of heads in the transformer multi attention thing.\n        nhid -> the number of hidden dimension neurons in the model.\n        nlayers -> how many layers we want to stack.\n        '''\n        print(params)\n        super(TransformerModel, self).__init__(**params)\n        self.pos_embedding = nn.Embedding(self.d_model, self.d_model) # positional embeddings\n        self.exercise_embeddings = nn.Embedding(num_embeddings=13523, embedding_dim=self.d_model) # exercise_id\n        self.part_embeddings = nn.Embedding(num_embeddings=7+1, embedding_dim=self.d_model) # part_id_embeddings\n        self.elapsed_time_embeddings = nn.Embedding(num_embeddings=301, embedding_dim=self.d_model) # prior_question_elapsed_time\n        self.target_embeddings = nn.Embedding(num_embeddings=3, embedding_dim=self.d_model)\n        self.linear = nn.Linear(self.d_model, 1)\n        self.device = DEVICE\n        \n        self.future_mask = self.generate_square_subsequent_mask(self.d_model).to(self.device)\n        self.init_weights()\n\n    def get_future_mask(self):\n        future_mask = np.triu(np.ones((self.d_model, self.d_model)), k=1).astype('bool')\n        return torch.from_numpy(future_mask)\n    \n    def init_weights(self):\n        initrange = 0.1\n        # init embeddings\n        self.exercise_embeddings.weight.data.uniform_(-initrange, initrange)\n        self.part_embeddings.weight.data.uniform_(-initrange, initrange)\n        self.elapsed_time_embeddings.weight.data.uniform_(-initrange, initrange)\n        self.target_embeddings.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, encoder_input, decoder_input, mask_pad=None):\n        '''\n        S is the sequence length, N the batch size and E the Embedding Dimension (number of features).\n        src: (S, N, E)\n        src_mask: (S, S)\n        src_key_padding_mask: (N, S)\n        padding mask is (N, S) with boolean True/False.\n        SRC_MASK is (S, S) with float(’-inf’) and float(0.0).\n        '''\n        embedded_src = (self.exercise_embeddings(encoder_input['content_id'])\n                        + self.pos_embedding(torch.arange(0, encoder_input['content_id'].shape[1])\n                                             .to(self.device).unsqueeze(0)\n                                             .repeat(encoder_input['content_id'].shape[0], 1))\n                        + self.part_embeddings(encoder_input['part'])\n                        + self.elapsed_time_embeddings(encoder_input['prior_question_elapsed_time'])\n                       ) # (N, S, E)\n        embedded_src = embedded_src.transpose(0, 1) # (S, N, E)\n        embedded_src = embedded_src * np.sqrt(self.d_model)\n\n        embedded_tgt = self.target_embeddings(decoder_input).transpose(0, 1)\n\n        output = super(TransformerModel, self).forward(src=embedded_src, tgt=embedded_tgt, \n                                                       tgt_mask=self.future_mask, \n                                                       src_key_padding_mask=mask_pad)\n\n        output = self.linear(output.transpose(1, 0)).squeeze(-1)\n\n        return output","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data():\n    return pd.read_parquet(PARQUETS_DIR + 'train_merged.parquet', columns=[KEY_FEATURE]+FEATURES+[TARGET])#.iloc[-1_000_000:]\n\ndef split_train_valid(dt, size):\n    uids = dt[KEY_FEATURE].unique()\n    n_val = int(len(uids) * size)\n    val_uids = np.random.choice(uids, n_val)\n    val = dt[dt[KEY_FEATURE].isin(val_uids)]\n    trn = dt.drop(val.index)\n    return trn, val\n\ndef preprocess(data):\n    data[\"prior_question_elapsed_time\"].fillna(0, inplace=True) # some random value fill in\n    data[\"prior_question_elapsed_time\"] = data[\"prior_question_elapsed_time\"] // 1000  \n    return data\n\ndef pad_batch(batch, batch_len=SEQ_LEN, pad_value=0):\n    shape = ((batch_len - batch.shape[0], 0),) + tuple((0, 0) for i in range(len(batch.shape)-1))\n    return np.pad(batch, shape, constant_values=pad_value)\n\ndef rolling_window(a, w):\n    s0, s1 = a.strides\n    m, n = a.shape\n    return np.lib.stride_tricks.as_strided(\n        a, \n        shape=(m-w+1, w, n), \n        strides=(s0, s0, s1))\n\ndef make_time_series(x, windows_size, pad_value=0):\n  x = np.pad(x, [[windows_size-1, 0], [0, 0]], constant_values=0)\n  x = rolling_window(x, windows_size)\n  return x\n\ndef create_model():\n    return TransformerModel(d_model=SEQ_LEN, nhead=4, num_encoder_layers=3, \n                             num_decoder_layers=3, dim_feedforward=128, dropout=0.3)\n\ndef load_model():\n    estimator = create_model()\n    if os.path.exists(MODEL_FILENAME):\n        filepath = MODEL_FILENAME\n    else:\n        filepath = MODELS_DIR + f'transformer/{VERSION}/{MODEL_FILENAME}'\n    estimator.load_state_dict(torch.load(filepath))\n    return estimator","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Riiid(torch.utils.data.Dataset):\n    def __init__(self, groups, seq_len, pad_value=0, max_samples_per_user=None):\n        self.groups = groups\n        self.uids = list(groups.groups)\n        self.seq_len = seq_len\n        self.pad_value = pad_value\n        self.max_samples_per_user = max_samples_per_user\n    def __len__(self):\n        return len(self.uids)\n    \n    def __getitem__(self, idx):\n        uid = self.uids[idx]\n        g = self.groups.get_group(uid).copy()\n        g['mask'] = 1\n        g['decoder_input'] = g[TARGET].shift(fill_value=START_TOKEN)\n        rolling_data = make_time_series(g.values, self.seq_len, pad_value=0)\n        rolling_data[:, :, -2] = np.logical_not(rolling_data[:, :, -2])\n        rolling_data[:, 0, -1] = START_TOKEN\n        n_sequences = len(rolling_data)\n        if self.max_samples_per_user is not None and n_sequences > self.max_samples_per_user:\n            idx = np.random.choice(np.arange(n_sequences), self.max_samples_per_user)\n            rolling_data = rolling_data[idx]\n#         print(1)\n        return rolling_data\n\ndef collate_fn(batch):\n#     print(2)\n    return np.concatenate(batch).transpose(2, 0, 1)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_epoch(estimator, train_iterator, optim, criterion, device=\"cpu\", batch_limit=128):\n    estimator.train()\n\n    tbar = tqdm(train_iterator)\n    num_corrects = 0\n    loss_sum = 0\n    batch_count = 0\n    sample_count = 0\n    \n    for batch in tbar:\n#         print(3)\n        encoder_input = {}\n        for i, feat in enumerate(FEATURES):\n            if DTYPES[feat] is int:\n                encoder_input[feat] = torch.Tensor(batch[i]).to(device).long()\n            elif DTYPES[feat] is bool:\n                encoder_input[feat] = torch.Tensor(batch[i]).to(device).bool()\n\n        decoder_input = torch.Tensor(batch[-1]).to(device).long()\n        mask = torch.Tensor(batch[-2]).to(device).bool()\n        labels = torch.Tensor(batch[-3]).to(device).long()\n            \n        n_samples = len(labels)\n        n_batches = int(np.ceil(n_samples / batch_limit))\n        for nbatch in range(n_batches):\n            start_idx = nbatch * batch_limit\n            end_idx = (nbatch + 1) * batch_limit\n\n            optim.zero_grad()\n            output = estimator(encoder_input={name: feat[start_idx: end_idx] for name, feat in encoder_input.items()}, \n                               decoder_input=decoder_input[start_idx: end_idx], \n                               mask_pad=mask[start_idx: end_idx])\n            batch_labels = labels[start_idx: end_idx].float()\n            loss = criterion(output, batch_labels)\n            loss.backward()\n            optim.step()\n\n            loss_sum += loss.item()\n            pred = (torch.sigmoid(output) >= 0.5).long()\n            num_corrects += (pred == batch_labels).sum().item()\n            batch_count += 1\n            sample_count += len(batch_labels)  \n\n            tbar.set_description(f'{nbatch+1}/{n_batches} | ' + 'trn_loss - {:.4f}'.format(loss_sum / batch_count))\n    \n    acc = num_corrects / sample_count\n    loss = loss_sum / batch_count\n    \n    return loss, acc      ","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def val_epoch(estimator, val_iterator, criterion, device=\"cpu\", batch_limit=128):\n    estimator.eval()\n\n    loss_sum = 0\n    batch_count = 0\n    num_corrects = 0\n    sample_count = 0\n    targets = []\n    outs = []\n\n    tbar = tqdm(val_iterator)\n    for batch in tbar:\n        encoder_input = {}\n        for i, feat in enumerate(FEATURES):\n            if DTYPES[feat] is int:\n                encoder_input[feat] = torch.Tensor(batch[i]).to(device).long()\n            elif DTYPES[feat] is bool:\n                encoder_input[feat] = torch.Tensor(batch[i]).to(device).bool()\n\n        decoder_input = torch.Tensor(batch[-1]).to(device).long()\n        mask = torch.Tensor(batch[-2]).to(device).bool()\n        labels = torch.Tensor(batch[-3]).to(device).long()\n            \n        n_samples = len(labels)\n        n_batches = int(np.ceil(n_samples / batch_limit))\n        for nbatch in range(n_batches):\n            start_idx = nbatch * batch_limit\n            end_idx = (nbatch + 1) * batch_limit\n            with torch.no_grad():\n                output = estimator(encoder_input={name: feat[start_idx: end_idx] for name, feat in encoder_input.items()}, \n                                   decoder_input=decoder_input[start_idx: end_idx], \n                                   mask_pad=mask[start_idx: end_idx])\n            batch_labels = labels[start_idx: end_idx].float()\n            loss = criterion(output, batch_labels)\n            loss_sum += loss.item()\n            batch_count += 1\n\n            pred = (torch.sigmoid(output) >= 0.5).long()\n            num_corrects += (pred == batch_labels).sum().item()\n            sample_count += len(batch_labels)\n            targets.extend(batch_labels.data.cpu().numpy())\n            outs.extend(output.data.cpu().numpy())\n\n            tbar.set_description(f'{nbatch+1}/{n_batches} | ' + 'val_loss - {:.4f}'.format(loss_sum / batch_count))                 \n        \n    acc = num_corrects / sample_count\n    auc = roc_auc_score(targets, outs)\n    loss = loss_sum / batch_count\n\n    return loss, acc, auc        \n        ","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_transformer(estimator, train, valid, lr=1e-3, epochs=10, n_user_batches=32, batch_limit=128, \n                      max_samples_per_user=100, device=\"cpu\", early_stopping=2, eps=1e-4, nworkers=4):\n    trn_dataset = Riiid(groups=train.groupby('user_id')[FEATURES+[TARGET]], \n                        seq_len=SEQ_LEN, \n                        pad_value=PAD_VALUE, \n                        max_samples_per_user=max_samples_per_user)\n    trn_dataloader = torch.utils.data.DataLoader(dataset=trn_dataset, \n                                                 batch_size=n_user_batches, \n                                                 collate_fn=collate_fn, \n                                                 num_workers=nworkers)\n\n    val_dataset = Riiid(groups=valid.groupby('user_id')[FEATURES+[TARGET]], \n                        seq_len=SEQ_LEN, pad_value=PAD_VALUE, \n                        max_samples_per_user=None)\n    val_dataloader = torch.utils.data.DataLoader(dataset=val_dataset, \n                                                 batch_size=n_user_batches, \n                                                 collate_fn=collate_fn, \n                                                 num_workers=nworkers)\n\n    optimizer = torch.optim.Adam(estimator.parameters(), lr=lr)\n    criterion = nn.BCEWithLogitsLoss()\n\n    estimator.to(device)\n    criterion.to(device)\n    \n    over_fit = 0\n    last_auc = 0\n    for epoch in range(epochs):\n        trn_loss, trn_acc = train_epoch(estimator, trn_dataloader, optimizer, criterion, device, batch_limit)\n        print(\"Training epoch {} - loss:{:.4f} - acc: {:.4f}\".format(epoch + 1, trn_loss, trn_acc))\n\n        val_loss, val_acc, val_auc = val_epoch(estimator, val_dataloader, criterion, device, batch_limit)\n        print(\"Validation epoch {} - loss: {:.4f} - acc: {:.4f}, auc: {:.6f}\".format(epoch + 1, val_loss, val_acc, val_auc))\n\n        if val_auc > last_auc + eps:\n            last_auc = val_auc\n            over_fit = 0\n            torch.save(estimator.state_dict(),  OUT_DIR + MODEL_FILENAME)\n        else:\n            over_fit += 1\n\n        if over_fit >= early_stopping:\n            print(\"early stop epoch \", epoch + 1)\n            break\n    \n    return estimator","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"retrain_transformer = 1\ncont = 0\nif retrain_transformer and not COMPILE:\n    data = preprocess(load_data())\n    df_train, df_valid = split_train_valid(data, 0.2)\n    del data\n    gc.collect()\n    \n    if cont:\n        model = load_model()\n    else:\n        model = create_model()\n    \n    model = train_transformer(model, df_train, df_valid,\n                              lr=1e-3,\n                              epochs=1,\n                              n_user_batches=32,\n                              batch_limit=512,\n                              max_samples_per_user=32,\n                              early_stopping=2,\n                              eps=1e-4,\n                              nworkers=1,\n                              device=DEVICE)\nelse:\n    model = load_model()","execution_count":null,"outputs":[{"output_type":"stream","text":"{'d_model': 100, 'nhead': 4, 'num_encoder_layers': 3, 'num_decoder_layers': 3, 'dim_feedforward': 128, 'dropout': 0.3}\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=5034.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1dd3942b16440ea8c5601667ecfa7bb"}},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"class RiiidTest(torch.utils.data.Dataset):\n    def __init__(self, data, queries, seq_len, pad_value=0):\n        self.data = data\n        self.dcols = {col: i for i, col in enumerate([KEY_FEATURE] + FEATURES + [TARGET])}\n        self.queries = queries\n        self.seq_len = seq_len\n        self.pad_value = pad_value\n        \n    def __len__(self):\n        return len(self.queries)\n    \n    def __getitem__(self, idx):\n        query = self.queries[[idx]]\n        uid = query[0, self.dcols[KEY_FEATURE]]\n        query = np.delete(query, self.dcols[KEY_FEATURE], axis=1)\n\n        encoder_data = self.data[self.data[:, self.dcols[KEY_FEATURE]] == uid][-self.seq_len+1:]\n        labels = encoder_data[:, self.dcols[TARGET]]\n        encoder_data = np.delete(encoder_data, [self.dcols[KEY_FEATURE], self.dcols[TARGET]], axis=1)\n        \n        if len(encoder_data) == 0:\n            encoder_data = query\n        else:\n            encoder_data = np.r_[encoder_data, query]\n        decoder_data = np.r_[2, labels]\n        mask = np.zeros(encoder_data.shape[0])\n\n        encoder_data = pad_batch(encoder_data, self.seq_len, self.pad_value)\n        decoder_data = pad_batch(decoder_data, self.seq_len, self.pad_value)\n        mask = pad_batch(mask, self.seq_len, 1)\n\n        \n        return np.c_[encoder_data, mask, decoder_data]\n\n#         return {'encoder_input': encoder_data.T,\n#                 'decoder_input': decoder_data,\n#                 'mask': mask}\n    \ndef collate_fn_test(batch):\n    return np.array(batch).transpose(2, 0, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_test(estimator, tst_iterator, device=\"cpu\"):\n    estimator.eval()\n\n    preds = []\n    for batch in tst_iterator:\n        encoder_input = {}\n        for i, feat in enumerate(FEATURES):\n            if DTYPES[feat] is int:\n                encoder_input[feat] = torch.Tensor(batch[i].astype(np.int64)).to(device).long()\n            elif DTYPES[feat] is bool:\n                encoder_input[feat] = torch.Tensor(batch[i].astype(bool)).to(device).bool()\n\n        decoder_input = torch.Tensor(batch[-1].astype(np.int64)).to(device).long()\n        mask = torch.Tensor(batch[-2].astype(bool)).to(device).bool()\n        \n        with torch.no_grad():\n            output = model(encoder_input=encoder_input, decoder_input=decoder_input, mask_pad=mask)\n        output = torch.sigmoid(output)\n        preds.extend(output.data.cpu().numpy())\n\n    return preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ADDED_FEATURES = [\n    'part'\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import psutil\n\ndef predict_batch(estimator, tst_batch, prev_batch, prev_data, batch_size=128):\n    all_cols = list(tst_batch.columns) + ADDED_FEATURES + [TARGET]\n    all_cols = dict(zip(all_cols, range(len(all_cols))))\n    used_cols = [all_cols[feat] for feat in [KEY_FEATURE] + FEATURES]\n\n    tst_batch = preprocess(tst_batch).values\n\n    if (prev_batch is not None) & (psutil.virtual_memory().percent<90):\n        print(psutil.virtual_memory().percent)\n        prev_batch = np.c_[prev_batch, eval(tst_batch[0, all_cols['prior_group_answers_correct']])]\n        prev_batch = prev_batch[prev_batch[:, all_cols['content_type_id']] == 0][:, used_cols + [all_cols[TARGET]]]\n        prev_data = np.r_[prev_data, prev_batch]\n\n    parts = np.apply_along_axis(lambda rid: TAGS_DF[rid[0]]['part'] if rid[0] in TAGS_DF else 0,\n                                axis=1, arr=tst_batch[:, [all_cols['content_id']]])\n    tst_batch = np.c_[tst_batch, parts]\n    prev_batch = tst_batch.copy()\n    \n    qrows = tst_batch[:, all_cols['content_type_id']] == 0\n#     tst_batch = tst_batch[tst_batch[:, all_cols['content_type_id']] == 0][:, used_cols]\n    tst_dataset = RiiidTest(prev_data, tst_batch[qrows][:, used_cols], SEQ_LEN)\n    tst_dataloader = torch.utils.data.DataLoader(dataset=tst_dataset, batch_size=batch_size, collate_fn=collate_fn_test, num_workers=8)\n    \n    preds = predict_test(estimator, tst_dataloader, DEVICE)\n        \n    tst_batch = np.c_[tst_batch, preds]\n    predictions = pd.DataFrame(tst_batch[:, [all_cols['row_id'], all_cols[TARGET]]],\n                               columns=['row_id', TARGET])\n        \n    return {'preds': predictions,\n            'prev_batch': prev_batch,\n            'prev_data': prev_data}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"local_test = 1\n\nTAGS_DF = pd.read_parquet(PARQUETS_DIR + 'tags.parquet').to_dict('index')\nprevious_data = preprocess(load_data()).groupby(KEY_FEATURE).tail(SEQ_LEN).values\nprevious_batch = None\nmodel.eval()\n\nif local_test and not COMPILE:\n    example_test = pd.read_csv(DATA_DIR + 'example_test.csv')\n    submission = pd.DataFrame(columns=['row_id', TARGET])\n\n    for gnum in tqdm(example_test['group_num'].unique()):\n        test_batch = example_test[example_test['group_num'] == gnum].copy()\n        preds, previous_batch, previous_data = predict_batch(model, test_batch, previous_batch, previous_data, batch_size=10).values()\n#         df = predict_batch(lgbm_model, test_batch, previous_batch)\n        submission = submission.append(preds)\n\n    submission = submission.reset_index(drop=True)\n    print(submission)\nelse:\n    env = riiideducation.make_env()\n    for test_batch, _ in tqdm(env.iter_test()):\n        preds, previous_batch, previous_data = predict_batch(model, test_batch, previous_batch, previous_data, batch_size=128).values()\n        env.predict(preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}