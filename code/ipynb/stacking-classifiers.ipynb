{"cells":[{"cell_type":"code","source":"# This comment is here to prevent notebook from disconnecting...\n# %% [code]\n# TODO:\n# Implement save parameter in train().\n\n# %% [code]\nRANDOM_SEED = 44\n\n# %% [code]\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport os\nimport gc\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LogisticRegression\n\nfrom functools import partial\nfrom bayes_opt import BayesianOptimization, SequentialDomainReductionTransformer\nfrom bayes_opt.logger import JSONLogger, ScreenLogger\nfrom bayes_opt.event import Events\nfrom bayes_opt.util import load_logs\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# %% [code]\nclass StackingClassifier(object):\n    def __init__(self, classifiers, base_classifier, nfolds):\n        self.classifiers = classifiers\n        self.nclassifiers = len(classifiers)\n        self.base_classifier = base_classifier\n        self.nfolds = nfolds\n        self._kf = KFold(n_splits=nfolds, shuffle=True, random_state=RANDOM_SEED)\n        \n    def train(self, xtrain, ytrain):\n        base_xtrn = np.zeros(shape=(xtrain.shape[0], self.nclassifiers))                               \n        for iclf, clf in enumerate(self.classifiers):\n            print(f'\\nTraining {type(clf).__name__} model.')\n            base_xtrn[:, iclf] = self._train_clf(clf, xtrain, ytrain)\n\n        print('\\nTraining base classifier...')\n        self.base_classifier.train(base_xtrn, ytrain)\n        print('Done.')\n                                      \n    def _train_clf(self, clf, xtrain, ytrain):\n        val_preds = np.zeros(shape=(xtrain.shape[0],))\n        for ifold, (trn_idx, val_idx) in enumerate(self._kf.split(xtrain)):\n            print(f'--Fold {ifold + 1}...')\n            xtrn = xtrain.loc[trn_idx]\n            ytrn = ytrain.loc[trn_idx]\n            xval = xtrain.loc[val_idx]\n            yval = ytrain.loc[val_idx]\n            \n            clf.train(xtrn, ytrn)\n            clf_preds = clf.predict(xval)\n            val_preds[val_idx] = clf_preds\n            print(\"    \tvalidation's auc\", metrics.roc_auc_score(yval, clf_preds))\n            \n        print('--Training on all data...')\n        clf.train(xtrain, ytrain)\n        return val_preds\n    \n    def predict(self, xtest):\n        base_xtest = np.zeros(shape=(xtest.shape[0], self.nclassifiers))\n        for iclf, clf in enumerate(self.classifiers):\n            base_xtest[:, iclf] = clf.predict(xtest)\n        return self.base_classifier.predict(base_xtest)\n    \n    def score(self, xtest, ytest):\n        preds = self.predict(xtest)\n        return metrics.roc_auc_score(ytest, preds)\n\n# %% [code]\nclass LGBMClassifier(object):\n    def __init__(self, params):\n        self.params = params\n        self.params['feature_fraction_seed'] = RANDOM_SEED\n        self.params['bagging_seed'] = RANDOM_SEED\n        self.log = {}\n        self.clf = None\n        self.best_score = 0\n        \n    def train(self, train, valid, verbose=1):\n        train = lgb.Dataset(train['x'], label=train['y'], free_raw_data=True)\n        valid = lgb.Dataset(valid['x'], label=valid['y'], free_raw_data=True, reference=train)\n        self.log = {}\n        self.clf = lgb.train(self.params, \n                             train,\n                             valid_sets=[valid],\n                             callbacks=[lgb.record_evaluation(self.log),\n                                        lgb.early_stopping(12, verbose=verbose)], \n                             verbose_eval=verbose)\n        self.best_score = max(self.log['valid_0'][opt_params['metric']])\n        \n    def predict(self, xtest):\n        return self.clf.predict(xtest)\n\n# %% [code]\nclass XGBClassifier(object):\n    def __init__(self, params):\n        self.params = params\n        self.params['seed'] = RANDOM_SEED\n        self.nrounds = params.pop('nrounds', 100)\n        self.clf = None\n        self.log = {}\n        self.best_score = 0\n        \n    def train(self, train, valid, verbose=1):\n        train = xgb.DMatrix(train['x'], label=train['y'])\n        valid = xgb.DMatrix(valid['x'], label=valid['y'])\n        self.log = {}\n        self.clf = xgb.train(self.params, \n                             train, \n                             evals=[(valid, 'valid')],\n                             num_boost_round=self.nrounds,\n                             early_stopping_rounds=12,\n                             verbose_eval=verbose)\n        self.best_score = self.clf.best_score\n        \n    def predict(self, xtest):\n        return self.clf.predict(xgb.DMatrix(xtest))\n\n# %% [code]\nclass CATBClassifier(object):\n    def __init__(self, params):\n        self.params = params\n        self.params['random_seed'] = RANDOM_SEED       \n        self.clf = None\n        self.best_score = 0\n\n    def train(self, train, valid, verbose=1):\n        self.clf = CatBoostClassifier(verbose=verbose, **self.params)\n        self.clf.fit(train['x'], train['y'],\n                     eval_set=(valid['x'], valid['y']),\n                     verbose=verbose,\n                     use_best_model=True,\n                     early_stopping_rounds=12)\n        self.best_score = self.clf.get_best_score()['validation'][self.params['eval_metric']]\n\n    def predict(self, xtest):\n        return self.clf.predict_proba(xtest)[:, 1]\n\n# %% [code]\nclass SklearnClassifier(object):\n    def __init__(self, params):\n        self.params = params\n        self.params['random_state'] = RANDOM_SEED\n        self.clf_type = self.params.pop('clf_type')\n        self.clf = None\n        self.best_score = 0\n        \n    def train(self, train, valid, verbose=0):\n        self.clf = self.clf_type(**self.params, verbose=verbose)\n        self.clf.fit(train['x'], train['y'])\n        self.best_score = self.clf.score(valid['x'], valid['y'])\n        \n    def predict(self, xtest):\n        return self.clf.predict_proba(xtest)[:, 1]\n\n# %% [code]\nclass NeuralNetworkClassifier(object):\n    def __init__(self, params):\n        self.params = params\n        self.clf = None\n        self.epochs = self.params.pop('epochs', 5)\n        self.batch_size = self.params.pop('batch_size', 5)\n        self.log = {}\n    def train(self, train, valid, verbose=2):\n        self.clf = self._create_ann(**self.params)\n        callback_es = EarlyStopping(monitor=f\"val_{self.params['metric']}\", \n                                    patience=10, mode='max', verbose=verbose)\n        self.log = self.clf.fit(train['x'], train['y'],\n                                epochs=self.epochs,\n                                batch_size=self.batch_size,\n                                validation_data=(valid['x'], valid['y']),\n                                callbacks=[callback_es],\n                                shuffle=True,\n                                verbose=verbose)\n\n    def predict(self, xtest):\n        return self.clf.predict(xtest)\n    \n    def _create_ann(in_shape,\n                    out_shape,\n                    metric,\n                    n_layers=1,\n                    last_units=64,\n                    units_decay=1,\n                    hidden_sizes=None,\n                    learning_rate=0.001, \n                    lr_decay=0.0,\n                    dropout_rate=0.1,\n                    norm=0):\n        \"\"\"\n        Creates a multilayer perceptron model.\n\n        :param in_shape: The shape of the input data.\n        :param out_shape: The shape of the model output.\n        :param n_layers: The number of hidden dense layers.\n        :param last_units: The number of units for the last hidden dense layer.\n        :param units_decay: The decay rate of the number of units going from last hidden layer to the first.\n        :param hidden_sizes: A list containing the number of nodes for each hidden dense layer. This is meant to be used as \n            an alternative to n_layers, n_units, and units_scale to specify the number of hidden layers and the number of \n            nodes per hidden layer.\n        :param learning_rate: The learning rate of the model.\n        :param lr_decay: The learning rate decay of the model.\n        :param dropout_rate: The dropout rate for the dropout layers.\n        :param norm: Whether to use a batch normalization layer after each dense layer or not.\n\n        :return An MLP model.\n        \"\"\"\n        kb.clear_session()\n        tf.compat.v1.reset_default_graph()\n\n        n_layers = int(np.floor(n_layers))\n        last_units = int(last_units)\n        norm = int(round(norm))\n        model = Sequential()\n\n        # Adding the input and hidden layers.\n        if hidden_sizes is not None:\n            for i_layer, i_units in enumerate(hidden_sizes):\n                if i_layer == 0:\n                    model.add(Dense(i_units, activation='relu',\n                                    input_shape=in_shape))\n                else:\n                    model.add(Dense(i_units, activation='relu'))                \n                if norm:\n                    model.add(BatchNormalization())\n                if dropout_rate:\n                    model.add(Dropout(dropout_rate)) \n        else:\n            for i_layer in range(n_layers):\n                if i_layer == 0:\n                    model.add(Dense(max(1, int(last_units*(units_decay**(n_layers-i_layer-1)))), \n                                    activation='relu', \n                                    input_shape=in_shape))\n                else:\n                    model.add(Dense(max(1, int(last_units*(units_decay**(n_layers-i_layer-1)))), \n                                    activation='relu'))                \n                if norm:\n                    model.add(BatchNormalization())\n                if dropout_rate:\n                    model.add(Dropout(dropout_rate)) \n\n        # Adding the output layer\n        model.add(Dense(units=out_shape, activation='sigmoid'))\n\n        model.compile(optimizer=optimizers.Adam(lr=learning_rate, decay=lr_decay), \n                      loss=\"binary_crossentropy\", metrics=metric)\n\n        return model\n\n# %% [code]\ndef get_scorer(clf, train, valid, fixed_params, opt_dtypes):\n    return partial(scorer, clf, train, valid, fixed_params, opt_dtypes)\n\ndef scorer(clf, train, valid, fixed_params, opt_dtypes, **opt_params):\n    for param in opt_params:\n        opt_params[param] = opt_dtypes[param](opt_params[param])\n    opt_params.update(fixed_params)\n    model = clf(opt_params)\n    model.train(train, valid, verbose=0)\n\n    return model.best_score\n\n# %% [code]\nclass Timer(object):\n    def __init__(self, engine):\n        engine.subscribe(Events.OPTIMIZATION_START, self)\n        engine.subscribe(Events.OPTIMIZATION_STEP, self)\n        self.init_time = None\n\n    def update(self, event, instance):\n        if event == Events.OPTIMIZATION_START:\n            self.init_time = time.time()\n        else:\n            time_taken = time.time() - self.init_time\n            print(\"Time Taken:\", time.strftime('%H:%M:%S', time.gmtime(time_taken)), end='\\r')\n            self.init_time = time.time()\n\ndef get_optimizer(scorer, params_grid, filename, cont_opt=False, probes=[], use_transformer=False):\n    transformer = SequentialDomainReductionTransformer(gamma_osc=0.8, eta=0.95)\n    optimizer = BayesianOptimization(f=scorer,\n                                     pbounds=params_grid,\n                                     verbose=2,\n                                     bounds_transformer=transformer if use_transformer else None)\n\n    scr_logger = ScreenLogger()\n    scr_logger._default_cell_size = 10\n    optimizer.subscribe(Events.OPTIMIZATION_START, scr_logger)\n    optimizer.subscribe(Events.OPTIMIZATION_STEP, scr_logger)\n    optimizer.subscribe(Events.OPTIMIZATION_END, scr_logger)    \n    \n    if cont_opt:\n        if os.path.exists(filename):\n            optimizer.dispatch(Events.OPTIMIZATION_START)\n            optimizer.unsubscribe(Events.OPTIMIZATION_START, scr_logger)\n            load_logs(optimizer, logs=[filename])\n        else:\n            print('File not found:', filename)\n    else:\n        for ps in probes:\n            optimizer.probe(lazy=True, params=ps)     \n        \n    f_logger = JSONLogger(path=filename, reset=False if cont_opt else True)\n    optimizer.subscribe(Events.OPTIMIZATION_STEP, f_logger)    \n    Timer(optimizer)\n\n    return optimizer","metadata":{"collapsed":false,"_kg_hide-input":false},"execution_count":0,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}